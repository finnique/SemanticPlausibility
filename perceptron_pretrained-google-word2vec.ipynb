{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary import\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import csv\n",
    "from nltk.tokenize import MWETokenizer, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_adept = \"datasets/adept/train-dev-test-split/\"\n",
    "path_pap = \"datasets/pap/train-dev-test-split/\"\n",
    "\n",
    "\n",
    "# adept\n",
    "\n",
    "def no_label(dictionary):\n",
    "    return {key: value for key, value in dictionary.items() if key!=\"label\"}\n",
    "\n",
    "\n",
    "with open(path_adept+\"/train.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    adept_train = json.load(file)    \n",
    "with open(path_adept+\"/val.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    adept_val = json.load(file)   \n",
    "with open(path_adept+\"/test.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    adept_test = json.load(file)\n",
    "\n",
    "\n",
    "# pap with multiclasses\n",
    "\n",
    "with open(path_pap + \"/multiclass/train.csv\") as file:\n",
    "    pap_multi_train = list(csv.DictReader(file))\n",
    "with open(path_pap + \"/multiclass/dev.csv\") as file:\n",
    "    pap_multi_dev = list(csv.DictReader(file))\n",
    "with open(path_pap + \"/multiclass/test.csv\") as file:\n",
    "    pap_multi_test = list(csv.DictReader(file))\n",
    "\n",
    "\n",
    "# pap with binary labels\n",
    "\n",
    "with open(path_pap + \"/binary/train.csv\") as file:\n",
    "    pap_bin_train = list(csv.DictReader(file))\n",
    "with open(path_pap + \"/binary/dev.csv\") as file:\n",
    "    pap_bin_dev = list(csv.DictReader(file))\n",
    "with open(path_pap + \"/binary/test.csv\") as file:\n",
    "    pap_bin_test = list(csv.DictReader(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-label splits \n",
    "\n",
    "# adept\n",
    "adept_train_data = [no_label(instance) for instance in adept_train]\n",
    "adept_train_labels = [instance['label'] for instance in adept_train]\n",
    "adept_val_data = [no_label(instance) for instance in adept_val]\n",
    "adept_val_labels = [instance['label'] for instance in adept_val]\n",
    "adept_test_data = [no_label(instance) for instance in adept_test]\n",
    "adept_test_labels = [instance['label'] for instance in adept_test]\n",
    "\n",
    "# pap with multiclasses\n",
    "pap_multi_train_data = [instance['text'] for instance in pap_multi_train]\n",
    "pap_multi_train_labels = [instance['label'] for instance in pap_multi_train]\n",
    "pap_multi_dev_data = [instance['text'] for instance in pap_multi_dev]\n",
    "pap_multi_dev_labels = [instance['label'] for instance in pap_multi_dev]\n",
    "pap_multi_test_data = [instance['text'] for instance in pap_multi_test]\n",
    "pap_multi_test_labels = [instance['label'] for instance in pap_multi_test]\n",
    "\n",
    "# pap with binary labels\n",
    "pap_bin_train_data = [instance['text'] for instance in pap_bin_train]\n",
    "pap_bin_train_labels = [instance['label'] for instance in pap_bin_train]\n",
    "pap_bin_dev_data = [instance['text'] for instance in pap_bin_dev]\n",
    "pap_bin_dev_labels = [instance['label'] for instance in pap_bin_dev]\n",
    "pap_bin_test_data = [instance['text'] for instance in pap_bin_test]\n",
    "pap_bin_test_labels = [instance['label'] for instance in pap_bin_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# general tokenizer\n",
    "def tokenize(sentences):\n",
    "    tokenized = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# tokenizer for ADEPT data with special token <sep>\n",
    "def tokenize_with_sep(sentences):\n",
    "    tokenizer = MWETokenizer()\n",
    "    tokenizer.add_mwe(('<', 'sep', '>'))\n",
    "    tokenized_with_sep = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return tokenized_with_sep\n",
    "\n",
    "# train CBOW model with vector size = 100\n",
    "def train_CBOW(data):\n",
    "    cbow_model = Word2Vec(min_count=1, vector_size=100, window=5)\n",
    "    cbow_model.build_vocab(data, progress_per=10000)\n",
    "    cbow_model.train(data, total_examples=cbow_model.corpus_count, epochs=100, report_delay=1)\n",
    "\n",
    "    return cbow_model\n",
    "\n",
    "\n",
    "# get sentence vector by calculate average word embeddings\n",
    "def get_sent_vec(model, sentence, google_pretrained=False):\n",
    "    \n",
    "     # if not use Google pretrained Word2Vec\n",
    "    if google_pretrained is False:\n",
    "        sent_len = len(sentence)\n",
    "        get_vec = [model.wv[word] if word in model.wv else np.zeros(100) for word in sentence]\n",
    "        sent_vec = np.sum(get_vec, axis=0) / sent_len\n",
    "    \n",
    "    # if use Google pretrained Word2Vec\n",
    "    else:\n",
    "        sent_len = len(sentence)\n",
    "        get_vec = [model[word] if word in model else np.zeros(300) for word in sentence]\n",
    "        sent_vec = np.sum(get_vec, axis=0) / sent_len\n",
    "\n",
    "    return sent_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load Google pretrained Word2Vec model\n",
    "google_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAP Dataset\n",
    "### Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get embeddings from our CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize\n",
    "pap_bin_train_tokenized = tokenize(pap_bin_train_data)\n",
    "# train CBOW for pap data\n",
    "pap_bin_CBOW = train_CBOW(pap_bin_train_tokenized)\n",
    "# get sentence vector\n",
    "pap_bin_train_sentence_vectors = [get_sent_vec(pap_bin_CBOW, sentence) for sentence in pap_bin_train_tokenized]\n",
    "\n",
    "# tokenize\n",
    "pap_multi_train_tokenized = tokenize(pap_multi_train_data)\n",
    "# train CBOW for pap data\n",
    "pap_multi_CBOW = train_CBOW(pap_multi_train_tokenized)\n",
    "# get sentence vector\n",
    "pap_multi_train_sentence_vectors = [get_sent_vec(pap_multi_CBOW, sentence) for sentence in pap_multi_train_tokenized]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings from Google Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pap_bin_train_sentence_vectors_gg =  [get_sent_vec(google_news, sentence, True) for sentence in pap_bin_train_tokenized]\n",
    "pap_multi_train_sentence_vectors_gg = [get_sent_vec(google_news, sentence, True) for sentence in pap_multi_train_tokenized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of the training set:\n",
      "pap binary model: 0.7123842592592593\n",
      "pap multi-class model: 0.33738425925925924\n",
      "=============================\n",
      "pap binary model (Google Word2Vec): 0.7407407407407407\n",
      "pap multi-class model (Google Word2Vec): 0.48900462962962965\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "# train perceptrons using embeddings from our CBOW model\n",
    "pap_bin_classifier = Perceptron(tol=1e-3, random_state=0)\n",
    "pap_bin_classifier.fit(pap_bin_train_sentence_vectors, pap_bin_train_labels)\n",
    "\n",
    "pap_multi_classifier = Perceptron(tol=1e-3, random_state=0)\n",
    "pap_multi_classifier.fit(pap_multi_train_sentence_vectors, pap_multi_train_labels)\n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "# train perceptrons using pretrained embedding from Google\n",
    "pap_bin_classifier_gg = Perceptron(tol=1e-3, random_state=0)\n",
    "pap_bin_classifier_gg.fit(pap_bin_train_sentence_vectors_gg, pap_bin_train_labels)\n",
    "\n",
    "pap_multi_classifier_gg = Perceptron(tol=1e-3, random_state=0)\n",
    "pap_multi_classifier_gg.fit(pap_multi_train_sentence_vectors_gg, pap_multi_train_labels)\n",
    "\n",
    "# print mean accuracy for train set\n",
    "print(\"Mean accuracy of the training set:\")\n",
    "print(f\"pap binary model: {pap_bin_classifier.score(pap_bin_train_sentence_vectors, pap_bin_train_labels)}\")\n",
    "print(f\"pap multi-class model: {pap_multi_classifier.score(pap_multi_train_sentence_vectors, pap_multi_train_labels)}\")\n",
    "print(\"=============================\")\n",
    "print(f\"pap binary model (Google Word2Vec): {pap_bin_classifier_gg.score(pap_bin_train_sentence_vectors_gg, pap_bin_train_labels)}\")\n",
    "print(f\"pap multi-class model (Google Word2Vec): {pap_multi_classifier_gg.score(pap_multi_train_sentence_vectors_gg, pap_multi_train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dev sets:\n",
      "pap binary model: 0.6712962962962963\n",
      "pap multi-class model: 0.2037037037037037\n",
      "=============================\n",
      "pap binary model (Google Word2Vec): 0.7175925925925926\n",
      "pap multi-class model (Google Word2Vec): 0.3287037037037037\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "# testing binary model with dev set\n",
    "pap_bin_dev_tokenized = tokenize(pap_bin_dev_data)\n",
    "pap_bin_dev_sentence_vectors = [get_sent_vec(pap_bin_CBOW, sentence) for sentence in pap_bin_dev_tokenized]\n",
    "pap_bin_dev_pred = pap_bin_classifier.predict(pap_bin_dev_sentence_vectors)\n",
    "\n",
    "# testing multi-class models with dev set\n",
    "pap_multi_dev_tokenized = tokenize(pap_multi_dev_data)\n",
    "pap_multi_dev_sentence_vectors = [get_sent_vec(pap_multi_CBOW, sentence) for sentence in pap_multi_dev_tokenized]\n",
    "pap_multi_dev_pred = pap_multi_classifier.predict(pap_multi_dev_sentence_vectors)\n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "# testing binary model with dev set\n",
    "pap_bin_dev_sentence_vectors_gg = [get_sent_vec(google_news, sentence, True) for sentence in pap_bin_dev_tokenized]\n",
    "pap_bin_dev_pred_gg = pap_bin_classifier_gg.predict(pap_bin_dev_sentence_vectors_gg)\n",
    "\n",
    "# testing multi-class models with dev set\n",
    "pap_multi_dev_sentence_vectors_gg = [get_sent_vec(google_news, sentence, True) for sentence in pap_multi_dev_tokenized]\n",
    "pap_multi_dev_pred_gg = pap_multi_classifier_gg.predict(pap_multi_dev_sentence_vectors_gg)\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy of dev sets:\")\n",
    "print(f'pap binary model: {accuracy_score(pap_bin_dev_labels, pap_bin_dev_pred)}')\n",
    "print(f'pap multi-class model: {accuracy_score(pap_multi_dev_labels, pap_multi_dev_pred)}')\n",
    "print(\"=============================\")\n",
    "print(f'pap binary model (Google Word2Vec): {accuracy_score(pap_bin_dev_labels, pap_bin_dev_pred_gg)}')\n",
    "print(f'pap multi-class model (Google Word2Vec): {accuracy_score(pap_multi_dev_labels, pap_multi_dev_pred_gg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test sets:\n",
      "pap binary model: 0.7222222222222222\n",
      "pap multi-class model: 0.18518518518518517\n",
      "=============================\n",
      "pap binary model (Google Word2Vec): 0.7083333333333334\n",
      "pap multi-class model (Google Word2Vec): 0.38425925925925924\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN THE FOLLOWING CODE FOR THE FINAL EVALUATION\n",
    "\n",
    "'''\n",
    "Outputs:\n",
    "    pap_bin_test_pred: predictions of binary classifier\n",
    "    pap_multi_test_pred: predictions of multi-classes classifier\n",
    "'''\n",
    "\n",
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "# testing binary model with test set\n",
    "pap_bin_test_tokenized = tokenize(pap_bin_test_data)\n",
    "pap_bin_test_sentence_vectors = [get_sent_vec(pap_bin_CBOW, sentence) for sentence in pap_bin_test_tokenized]\n",
    "pap_bin_test_pred = pap_bin_classifier.predict(pap_bin_test_sentence_vectors)\n",
    "\n",
    "# testing multi-class models with test set\n",
    "pap_multi_test_tokenized = tokenize(pap_multi_test_data)\n",
    "pap_multi_test_sentence_vectors = [get_sent_vec(pap_multi_CBOW, sentence) for sentence in pap_multi_test_tokenized]\n",
    "pap_multi_test_pred = pap_multi_classifier.predict(pap_multi_test_sentence_vectors)\n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "# testing binary model with test set\n",
    "pap_bin_test_sentence_vectors_gg = [get_sent_vec(google_news, sentence, True) for sentence in pap_bin_test_tokenized]\n",
    "pap_bin_test_pred_gg = pap_bin_classifier_gg.predict(pap_bin_test_sentence_vectors_gg)\n",
    "\n",
    "# testing multi-class models with test set\n",
    "pap_multi_test_sentence_vectors_gg = [get_sent_vec(google_news, sentence, True) for sentence in pap_multi_test_tokenized]\n",
    "pap_multi_test_pred_gg = pap_multi_classifier_gg.predict(pap_multi_test_sentence_vectors_gg)\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy of test sets:\")\n",
    "print(f'pap binary model: {accuracy_score(pap_bin_test_labels, pap_bin_test_pred)}')\n",
    "print(f'pap multi-class model: {accuracy_score(pap_multi_test_labels, pap_multi_test_pred)}')\n",
    "print(\"=============================\")\n",
    "print(f'pap binary model (Google Word2Vec): {accuracy_score(pap_bin_test_labels, pap_bin_test_pred_gg)}')\n",
    "print(f'pap multi-class model (Google Word2Vec): {accuracy_score(pap_multi_test_labels, pap_multi_test_pred_gg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADEPT Dataset\n",
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using both sentence 1 and 2, separated by <SEP> token\n",
    "def merge_sentence(data):\n",
    "    adept_sentences = [item['sentence1'] + \" <SEP> \" + item['sentence2'] for item in data]\n",
    "\n",
    "    return adept_sentences\n",
    "\n",
    "# using only sentence 2\n",
    "def extract_sentence2(data):\n",
    "    adept_sentences = [item['sentence2'] for item in data]\n",
    "\n",
    "    return adept_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get embeddings from our CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentence vectors with both sentences\n",
    "\n",
    "# merge sentence 1, and 2\n",
    "adept_train_sentences_merged = merge_sentence(adept_train_data)\n",
    "# tokenize\n",
    "adept_train_tokenized_merged = tokenize_with_sep(tokenize(adept_train_sentences_merged))\n",
    "# train CBOW for ADEPT data\n",
    "adept_CBOW_merged = train_CBOW(adept_train_tokenized_merged)\n",
    "# get sentence vector\n",
    "adept_train_sentence_vectors_merged = [get_sent_vec(adept_CBOW_merged, sentence) for sentence in adept_train_tokenized_merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentence vectors only with sentence 2\n",
    "\n",
    "# extract only sentence 2\n",
    "adept_train_sentences_single = extract_sentence2(adept_train_data)\n",
    "# tokenize\n",
    "adept_train_tokenized_single = tokenize(adept_train_sentences_single)\n",
    "# train CBOW for single sentence ADEPT data\n",
    "adept_CBOW_single = train_CBOW(adept_train_tokenized_single)\n",
    "# get sentence vector\n",
    "adept_train_sentence_vectors_single = [get_sent_vec(adept_CBOW_single, sentence) for sentence in adept_train_tokenized_single]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Embeddings from Google Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentence vectors with both sentences\n",
    "adept_train_sentence_vectors_merged_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_train_tokenized_merged]\n",
    "\n",
    "# get sentence vectors only with sentence 2\n",
    "adept_train_sentence_vectors_single_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_train_tokenized_single]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of the training set:\n",
      "adept multi-classes model (both sentences): 0.6111542041576171\n",
      "adept multi-classes model (only sentence 2): 0.5504964318957493\n",
      "=============================\n",
      "adept multi-classes model (both sentences, Google Word2Vec): 0.6694073844244492\n",
      "adept multi-classes model (only sentence 2, Google Word2Vec): 0.6747595408004964\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "adept_classifier_merged = Perceptron(tol=1e-3, random_state=0, early_stopping=True)\n",
    "adept_classifier_merged.fit(adept_train_sentence_vectors_merged, adept_train_labels)\n",
    "\n",
    "adept_classifier_single = Perceptron(tol=1e-3, random_state=0, early_stopping=True)\n",
    "adept_classifier_single.fit(adept_train_sentence_vectors_single, adept_train_labels) \n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "adept_classifier_merged_gg = Perceptron(tol=1e-3, random_state=0, early_stopping=True)\n",
    "adept_classifier_merged_gg.fit(adept_train_sentence_vectors_merged_gg, adept_train_labels)\n",
    "\n",
    "adept_classifier_single_gg = Perceptron(tol=1e-3, random_state=0, early_stopping=True)\n",
    "adept_classifier_single_gg.fit(adept_train_sentence_vectors_single_gg, adept_train_labels) \n",
    "\n",
    "# print mean accuracy for train set\n",
    "print(\"Mean accuracy of the training set:\")\n",
    "print(f\"adept multi-classes model (both sentences): {adept_classifier_merged.score(adept_train_sentence_vectors_merged, adept_train_labels)}\")\n",
    "print(f\"adept multi-classes model (only sentence 2): {adept_classifier_single.score(adept_train_sentence_vectors_single, adept_train_labels)}\")\n",
    "print(\"=============================\")\n",
    "print(f\"adept multi-classes model (both sentences, Google Word2Vec): {adept_classifier_merged_gg.score(adept_train_sentence_vectors_merged_gg, adept_train_labels)}\")\n",
    "print(f\"adept multi-classes model (only sentence 2, Google Word2Vec): {adept_classifier_single_gg.score(adept_train_sentence_vectors_single_gg, adept_train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "# using both sentences\n",
    "adept_val_sentences_merged = merge_sentence(adept_val_data)\n",
    "adept_val_tokenized_merged = tokenize_with_sep(tokenize(adept_val_sentences_merged))\n",
    "adept_val_sentence_vectors_merged = [get_sent_vec(adept_CBOW_merged, sentence) for sentence in adept_val_tokenized_merged]\n",
    "\n",
    "# using only sentence 2\n",
    "adept_val_sentences_single = merge_sentence(adept_val_data)\n",
    "adept_val_tokenized_single = tokenize(adept_val_sentences_single)\n",
    "adept_val_sentence_vectors_single = [get_sent_vec(adept_CBOW_single, sentence) for sentence in adept_val_tokenized_single]\n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "\n",
    "# using both sentences\n",
    "adept_val_sentence_vectors_merged_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_val_tokenized_merged]\n",
    "# using only sentence 2\n",
    "adept_val_sentence_vectors_single_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_val_tokenized_single]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of val sets:\n",
      "ADEPT multi-classes model (both sentences): 0.6058348851644941\n",
      "ADEPT multi-classes model (only sentence 2): 0.5127250155183116\n",
      "=============================\n",
      "ADEPT multi-classes model (both sentences, Google Word2Vec): 0.6635630043451273\n",
      "ADEPT multi-classes model (only sentence 2, Google Word2Vec): 0.6635630043451273\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "\n",
    "# test model with val set\n",
    "adept_val_pred_merged = adept_classifier_merged.predict(adept_val_sentence_vectors_merged)\n",
    "adept_val_pred_single = adept_classifier_single.predict(adept_val_sentence_vectors_single) \n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "adept_val_pred_merged_gg = adept_classifier_merged_gg.predict(adept_val_sentence_vectors_merged_gg)\n",
    "adept_val_pred_single_gg = adept_classifier_single_gg.predict(adept_val_sentence_vectors_single_gg) \n",
    "\n",
    "\n",
    "print(\"Accuracy of val sets:\")\n",
    "print(f'ADEPT multi-classes model (both sentences): {accuracy_score(adept_val_labels, adept_val_pred_merged)}')\n",
    "print(f'ADEPT multi-classes model (only sentence 2): {accuracy_score(adept_val_labels, adept_val_pred_single)}')\n",
    "print(\"=============================\")\n",
    "print(f'ADEPT multi-classes model (both sentences, Google Word2Vec): {accuracy_score(adept_val_labels, adept_val_pred_merged_gg)}')\n",
    "print(f'ADEPT multi-classes model (only sentence 2, Google Word2Vec): {accuracy_score(adept_val_labels, adept_val_pred_single_gg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test sets:\n",
      "ADEPT multi-classes model (with both sentences): 0.6240694789081885\n",
      "ADEPT multi-classes model (only sentence 2): 0.5688585607940446\n",
      "=============================\n",
      "ADEPT multi-classes model (with both sentences, Google Word2Vec): 0.6792803970223326\n",
      "ADEPT multi-classes model (only sentence 2, Google Word2Vec): 0.6892059553349876\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN THE FOLLOWING CODE FOR THE FINAL EVALUATION\n",
    "\n",
    "'''\n",
    "Outputs:\n",
    "    adept_test_pred_merged: predictions of adept multi-classes classifier (with both sentences)\n",
    "    adept_test_pred_single: predictions of adept multi-classes classifier (with only sentence 2)\n",
    "    adept_test_pred_merged_gg: predictions of adept multi-classes classifier (with both sentences, using Google Word2Vec)\n",
    "    adept_test_pred_single_gg: predictions of adept multi-classes classifier (with only sentence 2, using Google Word2Vec)\n",
    "'''\n",
    "\n",
    "'''\n",
    "Using our Embeddings from CBOW\n",
    "'''\n",
    "# get document vector for test set\n",
    "adept_test_sentences_merged = merge_sentence(adept_test_data)\n",
    "adept_test_tokenized_merged = tokenize_with_sep(tokenize(adept_test_sentences_merged))\n",
    "adept_test_sentence_vectors_merged = [get_sent_vec(adept_CBOW_merged, sentence) for sentence in adept_test_tokenized_merged]\n",
    "\n",
    "adept_test_sentences_single = extract_sentence2(adept_test_data)\n",
    "adept_test_tokenized_single = tokenize(adept_test_sentences_single)\n",
    "adept_test_sentence_vectors_single = [get_sent_vec(adept_CBOW_single, sentence) for sentence in adept_test_tokenized_single]\n",
    "\n",
    "# test models with test set\n",
    "adept_test_pred_merged = adept_classifier_merged.predict(adept_test_sentence_vectors_merged)\n",
    "adept_test_pred_single = adept_classifier_single.predict(adept_test_sentence_vectors_single)\n",
    "\n",
    "'''\n",
    "Using Embeddings from Google pretrained Word2Vec\n",
    "'''\n",
    "\n",
    "# get document vector for test set\n",
    "adept_test_sentence_vectors_merged_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_test_tokenized_merged]\n",
    "adept_test_sentence_vectors_single_gg = [get_sent_vec(google_news, sentence, True) for sentence in adept_test_tokenized_single]\n",
    "\n",
    "# test models with test set\n",
    "adept_test_pred_merged_gg = adept_classifier_merged_gg.predict(adept_test_sentence_vectors_merged_gg)\n",
    "adept_test_pred_single_gg = adept_classifier_single_gg.predict(adept_test_sentence_vectors_single_gg)\n",
    "\n",
    "print(\"Accuracy of test sets:\")\n",
    "print(f'ADEPT multi-classes model (with both sentences): {accuracy_score(adept_test_labels, adept_test_pred_merged)}')\n",
    "print(f'ADEPT multi-classes model (only sentence 2): {accuracy_score(adept_test_labels, adept_test_pred_single)}')\n",
    "print(\"=============================\")\n",
    "print(f'ADEPT multi-classes model (with both sentences, Google Word2Vec): {accuracy_score(adept_test_labels, adept_test_pred_merged_gg)}')\n",
    "print(f'ADEPT multi-classes model (only sentence 2, Google Word2Vec): {accuracy_score(adept_test_labels, adept_test_pred_single_gg)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
